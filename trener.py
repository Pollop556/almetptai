# train.py
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
import torch
import re
import os
import numpy as np
from sklearn.metrics import accuracy_score
import logging

# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
MODEL_NAME = "ai-forever/rugpt3small_based_on_gpt2"
CSV_FILE_PATH = "data_new.csv"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
OUTPUT_DIR = "./my_rugpt3_finetuned"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs("./logs", exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("./logs/training.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# ==================== –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê ====================
def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
    if pd.isna(text):
        return ""
    text = str(text)
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–∞–≤—ã—á–∫–∏ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
    text = re.sub(r'^"|"$', '', text)
    text = re.sub(r'[‚Äú‚Äù¬´¬ª]', '"', text)           # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–≤—ã—á–µ–∫
    text = re.sub(r'http\S+', '', text)           # –°—Å—ã–ª–∫–∏
    text = re.sub(r'[^\w\s\.\,\!\?\:\;\-\+\"\'\(\)\‚Äî]', ' ', text)
    text = re.sub(r'\s+', ' ', text)              # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\?+', '?', text)              # ?? -> ?
    text = re.sub(r'\!+', '!', text)              # !! -> !
    return text.strip()

def contains_russian(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É"""
    return bool(re.search(r'[–∞-—è–ê-–Ø]', text))

def calculate_average_length(dataset):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ max_length"""
    lengths = []
    for example in dataset:
        text = example['text'] if 'text' in example else example
        lengths.append(len(text.split()))
    return np.mean(lengths), np.max(lengths)

# ==================== –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================
def load_and_format_data(csv_path):
    print("üîç –ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞...")
    try:
        # –ß–∏—Ç–∞–µ–º –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        df = pd.read_csv(
            csv_path,
            header=None,
            names=['question', 'answer'],
            encoding='utf-8',
            on_bad_lines='skip'
        )
        print(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        print("\nüìã –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
        for i in range(min(5, len(df))):
            q = df.iloc[i]['question']
            a = df.iloc[i]['answer']
            print(f"  {i+1}. –í: {str(q)[:80]}...")
            print(f"     –û: {str(a)[:80]}...")
            print()
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}")
        return None

    dialog_examples = []
    skipped_no_text = 0
    skipped_english = 0
    skipped_short = 0
    length_stats = []

    for _, row in df.iterrows():
        q = clean_text(row['question'])
        a = clean_text(row['answer'])

        # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö
        if not q or not a:
            skipped_no_text += 1
            continue

        # –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        if not contains_russian(q):
            skipped_english += 1
            continue

        # –§–∏–ª—å—Ç—Ä —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö
        if len(q) < 3 or len(a) < 2:
            skipped_short += 1
            continue

        # –§–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
        dialog = f"–í–æ–ø—Ä–æ—Å: {q}\n–û—Ç–≤–µ—Ç: {a}"
        dialog_examples.append(dialog)
        length_stats.append(len(dialog.split()))

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(dialog_examples)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    print(f"‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: –ø—É—Å—Ç—ã—Ö={skipped_no_text}, –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö={skipped_english}, –∫–æ—Ä–æ—Ç–∫–∏—Ö={skipped_short}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤
    if length_stats:
        avg_length, max_length_val = np.mean(length_stats), np.max(length_stats)
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã: —Å—Ä–µ–¥–Ω—è—è={avg_length:.1f}, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è={max_length_val}")
        print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π max_length: {min(512, int(max_length_val * 1.2))}")

    dataset = Dataset.from_dict({"text": dialog_examples})
    return dataset.train_test_split(test_size=0.1, seed=42, shuffle=True)

# ==================== –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ====================
def main():
    print("=" * 60)
    print("üöÄ –ó–ê–ü–£–©–ï–ù –ü–†–û–¶–ï–°–° –î–û–û–ë–£–ß–ï–ù–ò–Ø RuGPT3")
    print(f"–ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"–î–∞–Ω–Ω—ã–µ: {CSV_FILE_PATH}")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    dataset = load_and_format_data(CSV_FILE_PATH)
    if dataset is None:
        return

    print(f"üìä –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset['train'])}")
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {len(dataset['test'])}")

    # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ max_length
    avg_length, max_length_val = calculate_average_length(dataset['train'])
    max_length = min(512, int(max_length_val * 1.3))  # –ó–∞–ø–∞—Å 30%
    print(f"üî§ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω max_length: {max_length}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\n‚è¨ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        model = model.to(device)
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        print(f"üí° –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(tokenizer)}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors=None,
        )
        # –î–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–∫–∏ —Ç–∞–∫–∏–µ –∂–µ –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã–µ ID
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=2,
        remove_columns=["text"]
    )

    # –ö–æ–ª–ª–∞—Ç–æ—Ä –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )

    # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 878 –ø—Ä–∏–º–µ—Ä–æ–≤
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=5,                      # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        per_device_train_batch_size=4,           # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,           # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch_size = 16
        learning_rate=3e-5,                      # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–ª–∏ learning rate
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_strategy="steps",
        logging_steps=50,
        save_strategy="epoch",
        eval_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_total_limit=3,
        report_to="none",
        fp16=torch.cuda.is_available(),          # –í–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å CUDA
        dataloader_num_workers=2,
        dataloader_pin_memory=True,
        remove_unused_columns=True,
        disable_tqdm=False,
        seed=42,
        prediction_loss_only=True,
        gradient_checkpointing=True,             # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    print("\nüìà –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê...")
    try:
        eval_results = trainer.evaluate()
        print(f"üìä –ù–∞—á–∞–ª—å–Ω—ã–π loss: {eval_results['eval_loss']:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É: {e}")

    # –û–±—É—á–µ–Ω–∏–µ
    print("\nüî• –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï...")
    try:
        train_result = trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        metrics = train_result.metrics
        print(f"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üìä Final train loss: {metrics.get('train_loss', 'N/A')}")
        print(f"üìä Final eval loss: {metrics.get('eval_loss', 'N/A')}")
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{OUTPUT_DIR}'")
        
        # –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        print("\nüß™ –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò...")
        test_questions = [
            "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —Å–ø—Ä–∞–≤–∫—É —Å –º–µ—Å—Ç–∞ —É—á–µ–±—ã?",
            "–ö–æ–≥–¥–∞ –±—É–¥—É—Ç –∫–∞–Ω–∏–∫—É–ª—ã?",
            "–ö–∞–∫ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –≤ IT-–∫—É–±?"
        ]
        
        for test_question in test_questions:
            prompt = f"–í–æ–ø—Ä–æ—Å: {test_question}\n–û—Ç–≤–µ—Ç:"
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=150,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"‚ùì –í–æ–ø—Ä–æ—Å: {test_question}")
            print(f"ü§ñ –û—Ç–≤–µ—Ç: {generated_text[len(prompt):]}")
            print("-" * 50)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()